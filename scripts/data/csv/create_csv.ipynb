{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy, stanza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 21:16:09 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 266MB/s]                     \n",
      "2024-04-29 21:16:09 INFO: Downloaded file to /Users/perrine/stanza_resources/resources.json\n",
      "2024-04-29 21:16:10 INFO: Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| mwt       | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "| ner       | aqmar_charlm  |\n",
      "=============================\n",
      "\n",
      "2024-04-29 21:16:10 INFO: Using device: cpu\n",
      "2024-04-29 21:16:10 INFO: Loading: tokenize\n",
      "2024-04-29 21:16:10 INFO: Loading: mwt\n",
      "2024-04-29 21:16:10 INFO: Loading: pos\n",
      "2024-04-29 21:16:10 INFO: Loading: lemma\n",
      "2024-04-29 21:16:10 INFO: Loading: depparse\n",
      "2024-04-29 21:16:10 INFO: Loading: ner\n",
      "2024-04-29 21:16:11 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp_ja = spacy.load('ja_core_news_sm')  # Japonais\n",
    "nlp_zh = spacy.load('zh_core_web_sm')   # Chinois\n",
    "nlp_ar = stanza.Pipeline(lang='ar')    # Arabe\n",
    "nlp_default = spacy.load('en_core_web_sm')  # Modèle par défaut pour les autres langues\n",
    "\n",
    "\n",
    "def segment_sentences(text: str, language: str) -> list:\n",
    "    \"\"\"\n",
    "    Segmenter un texte en phrases.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Le texte à segmenter.\n",
    "    language (str): La langue du texte.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: Une liste de phrases.\n",
    "    \"\"\"\n",
    "    if language == 'ar':\n",
    "        doc = nlp_ar(text)\n",
    "        return [sentence.text.strip() for sentence in doc.sentences]\n",
    "    else:\n",
    "        if language == 'ja':\n",
    "            doc = nlp_ja(text)\n",
    "        elif language == 'zh':\n",
    "            doc = nlp_zh(text)\n",
    "        else:   \n",
    "            doc = nlp_default(text)\n",
    "        return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "\n",
    "def create_csv(folder: str, folder_csv: str) -> None:\n",
    "    \"\"\"\n",
    "    Crée un fichier CSV à partir des fichiers texte dans le dossier donné.\n",
    "    Chaque ligne du fichier CSV contient une paire de phrases et la langue de ces phrases.\n",
    "    Les fichiers texte doivent être nommés selon le format suivant : \"langue_nom.txt\".\n",
    "\n",
    "    Parameters:\n",
    "    folder (str): Le chemin du dossier contenant les fichiers texte.\n",
    "    folder_csv (str): Le chemin du dossier où le fichier CSV sera enregistré.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    data_by_language = defaultdict(list)\n",
    "\n",
    "    for file in os.listdir(folder):\n",
    "        lang = file.split(\"_\")[0]\n",
    "        with open(os.path.join(folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        if text:\n",
    "            # Utiliser sent_tokenize pour diviser le texte en phrases\n",
    "            sentences = segment_sentences(text, lang)\n",
    "            # Ajouter des paires de phrases\n",
    "            if len(sentences) > 1:\n",
    "                for i in range(0, len(sentences) - 1, 2):  # S'arrêter à l'avant-dernière phrase pour éviter un index hors limites\n",
    "                    data_by_language[lang].append(' '.join(sentences[i:i+2]))\n",
    "            else:\n",
    "                data_by_language[lang].append(' '.join(sentences))\n",
    "    \n",
    "    balanced_data = []\n",
    "    for lang, pairs in data_by_language.items():\n",
    "        # Créer des tuples (langue, paire de phrases) pour le DataFrame\n",
    "        balanced_data.extend((lang, pair) for pair in pairs)\n",
    "\n",
    "\n",
    "    df_balanced = pd.DataFrame(balanced_data, columns=[\"Label\", \"Text\"])\n",
    "    os.makedirs(folder_csv, exist_ok=True)\n",
    "    df_balanced.to_csv(os.path.join(folder_csv, \"data.csv\"), index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je définis une fonction appelée `create_csv` qui prend deux paramètres : `folder` et `folder_csv`. `folder` est le chemin du dossier contenant les fichiers textuels que je veux analyser, et `folder_csv` est le chemin du dossier où je souhaite enregistrer le fichier CSV final.\n",
    "\n",
    "Voici comment je m'organise pour traiter les données :\n",
    "\n",
    "1. **Extraction et segmentation des phrases** :\n",
    "   - Je crée un dictionnaire `data_by_language` en utilisant `defaultdict(list)`. Cela permet de collecter des paires de phrases, organisées par langue.\n",
    "   - Je parcours tous les fichiers dans le dossier spécifié avec `os.listdir(folder)`. Pour chaque fichier, je détermine la langue en extrayant la première partie du nom du fichier (avant le premier '_'). Cela suppose que chaque fichier est nommé selon le format \"langue_nom.txt\".\n",
    "   - J'ouvre chaque fichier en lecture, en utilisant son chemin complet et l'encodage UTF-8. Je lis son contenu et utilise `sent_tokenize` pour diviser le texte en phrases. Je parcours ces phrases par paires et les stocke dans le dictionnaire sous la clé correspondant à la langue du fichier.\n",
    "\n",
    "2. **Préparation des données pour le CSV** :\n",
    "   - Après avoir traité tous les fichiers, je parcours le dictionnaire `data_by_language`. Pour chaque langue, je prépare des tuples composés de la langue et des paires de phrases pour créer un ensemble de données structuré.\n",
    "\n",
    "3. **Création du DataFrame et enregistrement en CSV** :\n",
    "   - Je convertis l'ensemble de données en un DataFrame Pandas nommé `df_balanced`, avec les colonnes \"Label\" pour la langue et \"Text\" pour le texte des paires de phrases.\n",
    "   - Je vérifie si le dossier `folder_csv` existe et le crée s'il n'existe pas avec `os.makedirs`.\n",
    "   - Je sauvegarde le DataFrame dans un fichier CSV sous le nom \"data.csv\" dans le dossier `folder_csv`, en m'assurant de ne pas inclure l'index du DataFrame dans le fichier CSV et d'utiliser l'encodage UTF-8.\n",
    "\n",
    "Cette méthode `create_csv` permet de structurer efficacement des paires de phrases par langue dans un fichier CSV, facilitant les analyses linguistiques ou autres traitements ultérieurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_csv(csv: str, folder_csv: str) -> None:\n",
    "    \"\"\"\n",
    "    Divise un fichier CSV en un ensemble d'entraînement et un ensemble de test.\n",
    "    Les ensembles d'entraînement et de test contiennent 80% et 20% des données respectivement.\n",
    "\n",
    "    Parameters:\n",
    "    csv (str): Le chemin du fichier CSV.\n",
    "    folder_csv (str): Le chemin du dossier où les fichiers CSV seront enregistrés.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    df_train, df_test = train_test_split(csv, test_size=0.2, random_state=42)\n",
    "    df_train.to_csv(os.path.join(folder_csv, \"data_train.csv\"), index=False, encoding=\"utf-8\")\n",
    "    df_test.to_csv(os.path.join(folder_csv, \"data_test.csv\"), index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je définis une fonction appelée `create_train_test_csv` qui prend deux paramètres : `csv` et `folder_csv`. Le premier, `csv`, est un DataFrame qui contient les données, tandis que `folder_csv` est le chemin du dossier où je souhaite enregistrer les fichiers CSV résultants pour les ensembles d'entraînement et de test.\n",
    "\n",
    "Voici comment je procède pour diviser les données et les enregistrer :\n",
    "\n",
    "1. **Division des données en ensembles d'entraînement et de test** :\n",
    "   - J'utilise la fonction `train_test_split` de la bibliothèque scikit-learn pour diviser le DataFrame `csv` en deux sous-ensembles. Je spécifie `test_size=0.2` pour allouer 20% des données au jeu de test, tandis que les 80% restants constitueront le jeu d'entraînement. L'argument `random_state=42` est utilisé pour garantir la reproductibilité du découpage des données.\n",
    "\n",
    "2. **Enregistrement des ensembles d'entraînement et de test en CSV** :\n",
    "   - Une fois les données divisées, je sauvegarde l'ensemble d'entraînement dans un fichier CSV nommé \"data_train.csv\" et l'ensemble de test dans un fichier \"data_test.csv\". Ces fichiers sont enregistrés dans le dossier spécifié par `folder_csv`.\n",
    "   - Pour chaque sauvegarde, je spécifie `index=False` pour ne pas inclure l'index du DataFrame dans le fichier CSV et `encoding='utf-8'` pour assurer que le fichier est encodé correctement, ce qui est important pour le traitement ultérieur des données, surtout si elles contiennent des caractères non ASCII.\n",
    "\n",
    "Cette fonction `create_train_test_csv` permet de préparer efficacement les données pour des phases ultérieures de modélisation en machine learning, en s'assurant que les données sont correctement réparties et enregistrées de manière à faciliter leur accès et leur utilisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    folder_csv = \"../../../data/csv/\"\n",
    "    folder_txt_files = \"../../../data/clean/\"\n",
    "    create_csv(folder_txt_files, folder_csv)\n",
    "    df_balanced = pd.read_csv(os.path.join(folder_csv, \"data.csv\"), encoding=\"utf-8\")\n",
    "    create_train_test_csv(df_balanced, folder_csv)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
