{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Présentation du notebook\n",
    "\n",
    "Ce notebook a été conçu pour transformer une collection de fichiers texte, chacun étiqueté par langue, en un fichier CSV contenant des paires de phrases segmentées.\n",
    "\n",
    "Fonctions principales :\n",
    "- `segment_sentences(text: str, language: str) -> list`: Segmente un texte donné en phrases selon la langue spécifiée en utilisant les modèles NLP appropriés.\n",
    "- `create_csv(folder: str, folder_csv: str, num_pairs: int = 550) -> None`: Regroupe des paires de phrases par langue et les écrit dans un fichier CSV équilibré.\n",
    "\n",
    "Le script suit les étapes suivantes :\n",
    "1. **Initialisation des modèles linguistiques** :\n",
    "   - Plusieurs modèles NLP (spacy et stanza) sont chargés pour traiter différentes langues : anglais, français, chinois, etc.\n",
    "\n",
    "2. **Segmenter un texte en phrases** :\n",
    "   - La fonction `segment_sentences` est utilisée pour diviser un texte en phrases en utilisant des modèles NLP spécifiques à chaque langue.\n",
    "\n",
    "3. **Création du fichier CSV** :\n",
    "   - La fonction `create_csv` lit les fichiers texte du dossier source, les segmente en phrases, regroupe les phrases par paires, et écrit ces paires dans un fichier CSV.\n",
    "   - Les fichiers texte sont identifiés par leurs noms, qui suivent un format indiquant la langue.\n",
    "\n",
    "4. **Équilibrage des données** :\n",
    "   - Un nombre fixe de paires est sélectionné pour chaque langue afin d'équilibrer les données dans le CSV final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy, stanza\n",
    "import jupyter_black\n",
    "from collections import defaultdict\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jupyter_black.load() # Charge l'extension jupyter-black pour formater le code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 10:38:19 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e80acf676340b8b7b1bdbd54836df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 10:38:20 INFO: Downloaded file to /Users/perrine/stanza_resources/resources.json\n",
      "INFO:stanza:Downloaded file to /Users/perrine/stanza_resources/resources.json\n",
      "2024-05-12 10:38:21 INFO: Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| mwt       | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "| ner       | aqmar_charlm  |\n",
      "=============================\n",
      "\n",
      "INFO:stanza:Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| mwt       | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "| ner       | aqmar_charlm  |\n",
      "=============================\n",
      "\n",
      "2024-05-12 10:38:21 INFO: Using device: cpu\n",
      "INFO:stanza:Using device: cpu\n",
      "2024-05-12 10:38:21 INFO: Loading: tokenize\n",
      "INFO:stanza:Loading: tokenize\n",
      "2024-05-12 10:38:21 INFO: Loading: mwt\n",
      "INFO:stanza:Loading: mwt\n",
      "2024-05-12 10:38:21 INFO: Loading: pos\n",
      "INFO:stanza:Loading: pos\n",
      "2024-05-12 10:38:22 INFO: Loading: lemma\n",
      "INFO:stanza:Loading: lemma\n",
      "2024-05-12 10:38:22 INFO: Loading: depparse\n",
      "INFO:stanza:Loading: depparse\n",
      "2024-05-12 10:38:22 INFO: Loading: ner\n",
      "INFO:stanza:Loading: ner\n",
      "2024-05-12 10:38:22 INFO: Done loading processors!\n",
      "INFO:stanza:Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp_en = spacy.load(\"en_core_web_sm\")  # English\n",
    "nlp_es = spacy.load(\"es_core_news_sm\")  # Spanish\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")  # German\n",
    "nlp_fr = spacy.load(\"fr_core_news_sm\")  # French\n",
    "nlp_ru = spacy.load(\"ru_core_news_sm\")  # Russian\n",
    "nlp_zh = spacy.load(\"zh_core_web_sm\")  # Chinese\n",
    "nlp_ja = spacy.load(\"ja_core_news_sm\")  # Japanese\n",
    "nlp_ko = spacy.load(\"ko_core_news_sm\")  # Korean\n",
    "nlp_ar = stanza.Pipeline(lang=\"ar\")  # Arabe\n",
    "\n",
    "\n",
    "def segment_sentences(text: str, language: str) -> list:\n",
    "    \"\"\"\n",
    "    Segmenter un texte en phrases.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Le texte à segmenter.\n",
    "    language (str): La langue du texte.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: Une liste de phrases.\n",
    "    \"\"\"\n",
    "    if language == \"ar\":\n",
    "        doc = nlp_ar(text)\n",
    "        return [sentence.text.strip() for sentence in doc.sentences]\n",
    "    else:\n",
    "        nlp = globals()[f\"nlp_{language}\"]\n",
    "        doc = nlp(text)\n",
    "        return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "\n",
    "def create_csv(folder: str, folder_csv: str, num_pairs: int = 550) -> None:\n",
    "    \"\"\"\n",
    "    Crée un fichier CSV équilibré à partir des fichiers texte dans le dossier donné.\n",
    "    Chaque ligne du fichier CSV contient une paire de phrases et la langue de ces phrases.\n",
    "    Les fichiers texte doivent être nommés selon le format suivant : \"langue_nom.txt\".\n",
    "\n",
    "    Parameters:\n",
    "    folder (str): Le chemin du dossier contenant les fichiers texte.\n",
    "    folder_csv (str): Le chemin du dossier où le fichier CSV sera enregistré.\n",
    "    num_pairs (int): Nombre de paires de phrases par langue.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    data_by_language = defaultdict(list)\n",
    "\n",
    "    for file in os.listdir(folder):\n",
    "        if file.startswith(\".\"):  # Cela ignore .DS_Store et d'autres fichiers cachés\n",
    "            continue\n",
    "\n",
    "        lang = file.split(\"_\")[0]\n",
    "        with open(os.path.join(folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        if text:\n",
    "            # Utiliser sent_tokenize pour diviser le texte en phrases\n",
    "            sentences = segment_sentences(text, lang)\n",
    "            # Ajouter des paires de phrases\n",
    "            if len(sentences) > 1:\n",
    "                for i in range(\n",
    "                    0, len(sentences) - 1, 2\n",
    "                ):  # S'arrêter à l'avant-dernière phrase pour éviter un index hors limites\n",
    "                    data_by_language[lang].append(\" \".join(sentences[i : i + 2]))\n",
    "            else:\n",
    "                data_by_language[lang].append(\" \".join(sentences))\n",
    "\n",
    "    balanced_data = []\n",
    "    for lang, pairs in data_by_language.items():\n",
    "        # Limiter à `num_pairs` paires en sélectionnant aléatoirement ou ajouter des paires vides si moins que nécessaire\n",
    "        if len(pairs) > num_pairs:\n",
    "            pairs = sample(pairs, num_pairs)\n",
    "        balanced_data.extend((lang, pair) for pair in pairs)\n",
    "\n",
    "    # Créer un DataFrame et sauvegarder en CSV\n",
    "    df_balanced = pd.DataFrame(balanced_data, columns=[\"Label\", \"Text\"])\n",
    "    os.makedirs(folder_csv, exist_ok=True)\n",
    "    df_balanced.to_csv(\n",
    "        os.path.join(folder_csv, \"data.csv\"), index=False, encoding=\"utf-8\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `create_csv` est conçue pour transformer une collection de fichiers textuels, chacun étiqueté par langue, en un fichier CSV contenant des paires de phrases. Voici les étapes détaillées du traitement implémenté par cette fonction :\n",
    "\n",
    "1. **Initialisation des structures de données** :\n",
    "   - Un dictionnaire `data_by_language` est initialisé avec `defaultdict(list)`. Ce dictionnaire va accumuler les phrases segmentées, regroupées par langue.\n",
    "\n",
    "2. **Traitement des fichiers texte** :\n",
    "   - La fonction parcourt tous les fichiers dans le dossier spécifié (`folder`). Elle ignore les fichiers cachés (comme `.DS_Store`).\n",
    "   - Pour chaque fichier valide, la langue est déterminée à partir de la partie initiale du nom du fichier (séparée par `_`). Par exemple, un fichier nommé \"fr_texte.txt\" serait associé à la langue française (`fr`).\n",
    "   - Chaque fichier est lu pour en extraire le texte intégral, et ce texte est ensuite segmenté en phrases à l'aide de la fonction `segment_sentences`, qui utilise des modèles de traitement du langage naturel spécifiques à chaque langue pour une segmentation précise.\n",
    "\n",
    "3. **Construction de paires de phrases** :\n",
    "   - Les phrases sont groupées par paires consécutives. Si le nombre total de phrases est impair, la dernière phrase reste seule. Ces paires (ou parfois une seule phrase) sont ensuite stockées dans le dictionnaire `data_by_language` sous la clé correspondante à la langue.\n",
    "\n",
    "4. **Équilibrage et préparation des données pour le CSV** :\n",
    "   - Après avoir collecté les paires de phrases pour toutes les langues, la fonction ajuste le nombre de paires pour chaque langue à `num_pairs`, en utilisant la fonction `sample` pour réduire ou maintenir le nombre de paires à ce seuil.\n",
    "   - Un tableau de données est créé pour chaque langue, contenant les paires de phrases avec l'étiquette de langue correspondante.\n",
    "\n",
    "5. **Création du DataFrame et enregistrement en CSV** :\n",
    "   - Les données sont assemblées dans un DataFrame Pandas nommé `df_balanced` avec les colonnes \"Label\" pour la langue et \"Text\" pour les paires de phrases.\n",
    "   - Le dossier de sortie (`folder_csv`) est vérifié et créé si nécessaire avec `os.makedirs`.\n",
    "   - Le DataFrame est enregistré dans un fichier CSV nommé \"data.csv\" dans le dossier `folder_csv`, sans inclure les indices du DataFrame et en utilisant l'encodage UTF-8 pour assurer la compatibilité des caractères.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier CSV créé avec succès.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    folder_csv = \"../../../data/csv/\"\n",
    "    folder_txt_files = \"../../../data/clean/\"\n",
    "    create_csv(folder_txt_files, folder_csv)\n",
    "\n",
    "    print(\"Fichier CSV créé avec succès.\")\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
